{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28d5a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, ModernBertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModernBertClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, model_name):\n",
    "        super().__init__()\n",
    "        self.bert = ModernBertModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return self.classifier(outputs.last_hidden_state[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b88d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dill\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_quantized = torch.load(r'C:\\Users\\Kulde\\OneDrive\\Desktop\\OFFICE\\Emotion_Sentiment_Analysis\\models\\CLEANED\\model_dill_2_layer_Cleandata.pth', pickle_module=dill,map_location=torch.device('cpu'))\n",
    "# model = torch.load(r'C:\\Users\\Kulde\\OneDrive\\Desktop\\OFFICE\\Emotion_Sentiment_Analysis\\models\\model_experiment_2\\modernbert_sentiment_model.pth', weights_only=False, map_location=torch.device('cpu'))\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27eb76f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): ModernBertClassifier(\n",
       "    (bert): ModernBertModel(\n",
       "      (embeddings): ModernBertEmbeddings(\n",
       "        (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): ModernBertEncoderLayer(\n",
       "          (attn_norm): Identity()\n",
       "          (attn): ModernBertAttention(\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "            (rotary_emb): ModernBertRotaryEmbedding()\n",
       "            (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_drop): Identity()\n",
       "          )\n",
       "          (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): ModernBertMLP(\n",
       "            (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "            (act): GELUActivation()\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "            (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1-21): 21 x ModernBertEncoderLayer(\n",
       "          (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ModernBertAttention(\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "            (rotary_emb): ModernBertRotaryEmbedding()\n",
       "            (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_drop): Identity()\n",
       "          )\n",
       "          (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): ModernBertMLP(\n",
       "            (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "            (act): GELUActivation()\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "            (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b54b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded:\n"
     ]
    }
   ],
   "source": [
    "print(\"Model loaded:\")\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "# Load the PDF file\n",
    "reader = PdfReader(r\"C:\\Users\\Kulde\\OneDrive\\Desktop\\RESUME\\KULDIP_PANCHAL.pdf\")\n",
    "\n",
    "# Extract text from all pages\n",
    "for page in reader.pages:\n",
    "    print(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1598d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8e57ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Neutral'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "TORCH_LOGS=\"+dynamo\"  \n",
    "TORCHDYNAMO_VERBOSE=1\n",
    "\n",
    "text = \"\"\"The room is quiet and orderly. The clock on the wall ticks steadily, marking the passage of time without hurry or delay. The curtains are open, allowing natural light to filter in, illuminating the space evenly. The air is neither warm nor cool, maintaining a comfortable temperature.\n",
    "\n",
    "Outside, the weather is calm. The sky is a clear blue, with a few wisps of clouds drifting lazily. The trees stand tall and still, their leaves rustling gently in the occasional breeze. The street is neither busy nor deserted, with a steady flow of people going about their daily routines.\n",
    "\n",
    "You sit at your desk, focused on the task at hand. The work is neither exciting nor boring, simply a series of steps to be completed methodically. You check items off your to-do list, one by one, without rush or procrastination. The environment is conducive to productivity, with minimal distractions.\n",
    "\n",
    "Throughout the day, you interact with colleagues and clients, exchanging information and updates. The conversations are polite and professional, devoid of personal emotions or biases. Decisions are made based on facts and data, with a clear understanding of the goals and objectives.\n",
    "\n",
    "Lunch is a simple affair, a balanced meal consumed at a reasonable pace. The cafeteria is neither crowded nor empty, with a hum of quiet conversation filling the air. You return to your desk, refreshed and ready to continue your work.\n",
    "\n",
    "As the day progresses, you complete your tasks efficiently. There are no surprises or disruptions, just a steady flow of work. The office environment is neutral, with a focus on getting things done without unnecessary drama or excitement.\n",
    "\n",
    "By the end of the day, you have accomplished what you set out to do. You leave the office, satisfied with your productivity. The commute home is uneventful, with traffic flowing smoothly. You arrive home, ready to relax and prepare for the next day.\n",
    "\n",
    "The evening is spent in quiet reflection, planning for the future without dwelling on the past. You go to bed at a reasonable hour, ready to face whatever the next day brings with a calm and composed demeanor.\"\"\"\n",
    "text = re.sub(r'\\s+',' ',text).strip().lower()\n",
    "text = re.sub(r'[^a-z0-9\\s]','',text)\n",
    "\n",
    "inputs = tokenizer(text, padding=True, truncation=True, max_length=1500, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_quantized(**inputs)\n",
    "    predicted_class = torch.argmax(outputs, dim=1)\n",
    "\n",
    "print(\"Predicted class:\", predicted_class.item())\n",
    "\n",
    "emotions = ['Aggresive','Fear','Happy','Neutral','Sad']\n",
    "predicted_label = emotions[predicted_class]\n",
    "predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd821783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.7859, -4.8227,  1.6116,  0.0065, -4.0654]])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3aab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ad46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sad'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61149923",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModernBertClassifier' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probabilities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get the predicted class label\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mid2label[predicted_class]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kulde\\anaconda3\\envs\\rag\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModernBertClassifier' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "logits = outputs\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "# Get the predicted class (index of the highest probability)\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "# Get the predicted class label\n",
    "predicted_label = model.config.id2label[predicted_class]\n",
    "# Print the results\n",
    "print(f\"Input text: {text}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")\n",
    "print(f\"Predicted class label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dab2964b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "RESUME instruction not found in code",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Move inputs to the same device as the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# inputs = {key: value.to(model.device) for key, value in inputs.items()}\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get the logits from the model output\u001b[39;00m\n\u001b[0;32m     16\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\Users\\Kulde\\anaconda3\\envs\\rag\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kulde\\anaconda3\\envs\\rag\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Kulde\\anaconda3\\envs\\rag\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[0;32m    571\u001b[0m )\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[0;32m    578\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[0;32m    579\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Kulde\\anaconda3\\envs\\rag\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kulde\\anaconda3\\envs\\rag\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Kulde\\anaconda3\\envs\\rag\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1322\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[1;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m   1320\u001b[0m is_skipfile \u001b[38;5;241m=\u001b[39m trace_rules\u001b[38;5;241m.\u001b[39mcheck(frame\u001b[38;5;241m.\u001b[39mf_code)\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m13\u001b[39m):\n\u001b[1;32m-> 1322\u001b[0m     has_started_execution \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_lasti \u001b[38;5;241m>\u001b[39m \u001b[43mfirst_real_inst_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1324\u001b[0m     has_started_execution \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_lasti \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m first_real_inst_idx(frame\u001b[38;5;241m.\u001b[39mf_code)\n",
      "File \u001b[1;32mc:\\Users\\Kulde\\anaconda3\\envs\\rag\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1290\u001b[0m, in \u001b[0;36mfirst_real_inst_idx\u001b[1;34m(code)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRESUME\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1289\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m-> 1290\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRESUME instruction not found in code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: RESUME instruction not found in code"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "\n",
    "    # Example input text\n",
    "    text = \"I am so happy today!\"\n",
    "    # Tokenize the input text   \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1500)\n",
    "    # Move inputs to the same device as the model\n",
    "    # inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "    # Forward pass through the model\n",
    "    outputs = model(**inputs)\n",
    "    # Get the logits from the model output\n",
    "    logits = outputs.logits\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    # Get the predicted class (index of the highest probability)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "    # Get the predicted class label\n",
    "    predicted_label = model.config.id2label[predicted_class]\n",
    "    # Print the results\n",
    "    print(f\"Input text: {text}\")\n",
    "    print(f\"Predicted class index: {predicted_class}\")\n",
    "    print(f\"Predicted class label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
